## JetBrains AI Code Completion Internship
### Task
One of the most widespread applications of natural language processing is to correct erroneous texts, in particular incorrectly spelled words. Your goal is to explore the existing spell checking tools and evaluate them.

We suggest the following steps for this test assignment:
* Collect the data for evaluation, e.g. you can use publicly available datasets, generate synthetic data, combine different approaches – it's up to you.
* Research and choose the metrics you want to use for evaluation.
* Explore the existing spell checkers (e.g. libraries, fine-tuned models, LLMs), pick some you like and evaluate them on the created test set.
* Analyse the results and think about the strengths and weaknesses of each spell checker, as well as the reasons for that. Think about how they could be improved.

As a result, you should provide a link to a repository containing the source code and documentation, e.g.:
* Scripts containing data preparation and evaluation
* Documentation describing datasets, tools and metrics you used
* A short report that describes your approach and ideas summarises the results and discusses the challenges you encountered
* Instructions on how to run and locally reproduce the results

#### This project focuses on evaluating multiple spelling correction libraries, analyzing their performance on various types of input data. The primary objectives were to generate, compare, and visualize the results of each library based on key metrics, and to assess their suitability for spell-checking tasks in software development and daily use contexts.

### Dataset Structure
The dataset used for evaluation is specifically designed to cover a wide variety of spelling errors relevant to programming and everyday text, file is `test.csv`. 
* Random Sentences: Contains general-purpose sentences with common misspellings. Source: https://www.kaggle.com/datasets/samarthagarwal23/spelling-mistake-data-1mn/data.
* Programming Commands: Includes code-like strings to test the libraries' handling of technical terms.
* Single Words: Consists of isolated, commonly misspelled words to measure word-level correction accuracy.
* Numbers in Words: Contains numbers spelled out in text form, testing the libraries’ recognition of numerical context.
* Names: Includes common names from people

### Metrics
The key metrics used for evaluation are as follows:
* Levenshtein Distance: Measures the edit distance between the Prediction and Correct columns. Lower values indicate better accuracy.
* Accuracy: Metric to measure how many accurate predictions are made by the match the expected correct sentences
* Exact Match: Overall binary indicator (1 for exact match, 0 otherwise) calculated for each entry in the dataset.

### Result Dataset Columns
* Index: Row identifier.
* Incorrect: The input text with intentional spelling errors.
* Correct: The correct version of the input text.
* Prediction: The output generated by each spell-checking library.
* Levenshtein Distance: Measures the difference between Prediction and Correct.
* Accuracy: Indicates whether the prediction exactly matches the correct answer by percentage.
* Exact Match: Binary indicator (True or False) of exact match between Prediction and Correct.

### Libraries
* Spell-check libraries: Pyspellchecker, TextBlob, SymSpell, PyEnchant.
* Other: Pandas, Seaborn, Matplotlib, NLTK (natural language toolkit).

### Approach
* Data Preparation: Each input entry was preprocessed to introduce intentional spelling errors across various types, as outlined in the dataset structure.
* Execution of Libraries: Each spell-checking library was tested on the same dataset, and predictions were stored for comparison.
* Metric Calculation: Levenshtein distance, accuracy, and exact match scores were computed for each library prediction.
* Visualization: Detailed visualizations were generated to highlight the strengths and weaknesses of each library across different data types. The visualizations included box plots to display average accuracy and Levenshtein distance, and histograms to illustrate the distribution of each metric by data type.
* Result Summary: Each library's performance was summarized in terms of average metric scores. Specific attention was given to data type-specific performance variations, helping identify cases where each library is more or less effective.

### Results and Discussion
* Overall Accuracy: SymSpell had slight advance in overall accuracy compared to other libraries.
* Handling Code: TextBlob did significantly better on code examples compared to other 3 libraries
* Exact Match Rates: TextBlob also had better results based on exact match metrics.
* Library for Random Sentences: It was close call between PySpellChecker and SymSpell, but SymSpell had better accuracy.
* Single Word Results: PySpellChecker and SymSpell had similar results that are slightly better than other 2 libraries.
* Human Names: PyEnchant was the only library that had acceptable results regarding this type of data.
* Levenstein Distance: All libraries had similar results for this metric, SymSpell had slightly better outcome.

##### Example of library result graph:
![image](https://github.com/user-attachments/assets/e46e844e-2441-4502-8c8e-69faa99ea950)

### Potential enhancements of the project
* Implementation of fine-tuned models, like for example: BERT, T5 or GPT models. Approach would be similar, we would execute models on our `test.csv` dataset and generate result dataset from which we would generate graphs.
* Also, implementation of LLMs through API, like OpenAI, Cohere or other LLMs.
* Add complexity to metrics and testing dataset. Potentially measure time efficiency.
* Enhance the result graphs to provide more advanced and diverse visualizations.

###### Note: enhancements are not implemented due to time constraint
